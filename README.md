# Deepchecks LLM Evaluation - Demos

Hands-on demos showing how to evaluate, debug, and monitor LLM apps with [Deepchecks](https://app.llm.deepchecks.com).

## Demos

| Demo | Description | Framework |
|---|---|---|
| [Content Creator Crew](content_creator_crew/) | Evaluate and debug a multi-agent content creation workflow | CrewAI |

## About Deepchecks

Deepchecks is a comprehensive solution for AI evaluations, helping you make sure your AI applications work properly from PoC to production.

**Deepchecks LLM Evaluation** is for testing, validating and monitoring LLM-based applications. With Deepchecks you can continuously validate LLM-based applications including characteristics, performance metrics, and potential pitfalls throughout the entire lifecycle from pre-deployment and internal experimentation to production.

Deepchecks' platform streamlines configuring auto-scoring and applying it to your LLM-based app during dev, CI/CD and production, making LLM evaluations achivable at scale. 

Explore the [full documentation](https://llmdocs.deepchecks.com/docs/what-is-deepchecks).

## Links

- [Deepchecks](https://app.llm.deepchecks.com)
- [Documentation](https://llmdocs.deepchecks.com)
- [API Reference](https://llmdocs.deepchecks.com/reference)
- [Blog](https://www.deepchecks.com/blog/)

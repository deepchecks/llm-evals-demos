# Deepchecks LLM Evaluation — Demos

Hands-on demos showing how to evaluate, debug, and monitor LLM apps with [Deepchecks](https://app.llm.deepchecks.com).

## Demos

| Demo | Description | Framework |
|---|---|---|
| [Content Creator Crew](content_creator_crew/) | Evaluate and debug a multi-agent content creation workflow | CrewAI |

## About Deepchecks

Deepchecks is a comprehensive solution for AI evaluation — helping you make sure your AI applications work properly from PoC to production.

**Deepchecks LLM Evaluation** is for testing, validating and monitoring LLM-based apps. With Deepchecks you can continuously validate LLM-based applications including characteristics, performance metrics, and potential pitfalls throughout the entire lifecycle from pre-deployment and internal experimentation to production.

LLM applications have no test set, causing a measurement problem. Without standardized performance metrics, credible auto-scoring, and streamlined version comparisons, every evaluation task becomes "a project" — slowing down innovation and increasing risk.

Deepchecks' platform streamlines configuring auto-scoring and applying it to your LLM-based app during dev, CI/CD and production:

- **Compare versions** of prompts, models, agents, & AI systems
- **Set up an auto-scoring pipeline**, addressing nuanced constraints
- **Generate datasets and create LLM judges** within minutes
- **Leverage auto-scoring** for annotations and data slicing & dicing
- **Test LLM apps within CI/CD** and monitor them in production

## Getting Started

1. [Sign up](https://app.llm.deepchecks.com) for Deepchecks
2. Pick a demo from the table above and follow along
3. Explore the [full documentation](https://llmdocs.deepchecks.com/docs/what-is-deepchecks) for more

## Links

- [Deepchecks](https://app.llm.deepchecks.com)
- [Documentation](https://llmdocs.deepchecks.com)
- [API Reference](https://llmdocs.deepchecks.com/reference)
- [Blog](https://www.deepchecks.com/blog/)
